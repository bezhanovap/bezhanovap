{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки, необходимые для работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\roum-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roum-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\roum-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Отключим оповещения\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Зададим константы, которые будем использовать во всём проекте.\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Загрузим данные из файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Ознакомимся с данными, посмотрим основную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущенных значений нет, типы данных соответствуют."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликатов нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Подготовим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159292"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_comments['Unnamed: 0'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В столбце 'Unnamed: 0' содержатся только уникальные значения и они дублируют индекс, поэтому удалим его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ознакомимся с классами, которые предстоит предсказывать модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансировааны, преобладает класс со значением 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.toxic.value_counts()/toxic_comments.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные подготовлены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Подготовим признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся функциями для очистки текста и его лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text_clear = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    list_clear = text_clear.split()\n",
    "    text = \" \".join(list_clear)\n",
    "    return text\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Lemmatize a Sentence with the appropriate POS tag\n",
    "    # lemm_list = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text) if w not in string.punctuation]\n",
    "    lemm_list = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(clear_text(text))]\n",
    "    lemm_text = \" \".join(lemm_list)\n",
    "    return lemm_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним подготовку признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7dc94bb0254d0b9f62b84e584296e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14min 24s\n",
      "Wall time: 31min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df_test['lemm_text'] = df_test.text.apply(lemmatize_text)\n",
    "toxic_comments['lemm_text'] = toxic_comments.text.progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Выделим целевой признак и разделим данные на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = toxic_comments.lemm_text\n",
    "target = toxic_comments.toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём выборку на обучающую и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=.25, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (119469,)\n",
      "X_test: (39823,)\n",
      "y_train: (119469,)\n",
      "y_test: (39823,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили данные и проверили их.\n",
    "\n",
    "Пропущенных значений и дубликатов нет.\n",
    "\n",
    "Обнаружили столбец 'Unnamed: 0', который оказался лишним, его удалили.\n",
    "\n",
    "Классы в данных оказались несбалансированными.\n",
    "\n",
    "Полученную выборку использовали для подготовки признаков.\n",
    "\n",
    "Текст комментариев был очищен от лишних символов и произведена его токенизация и лемматизация.\n",
    "\n",
    "После этого данные были разделены на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модели и выберем лучшую."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_F1: 0.7563296809250455\n",
      "best_params: {'logisticregression__max_iter': 200, 'logisticregression__C': 10}\n",
      "CPU times: total: 9.78 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer =  TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# model_rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "model_lr = LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "pipline_lr = make_pipeline(vectorizer, model_lr)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'logisticregression__C': range(10, 25, 1),\n",
    "    'logisticregression__max_iter': range(200, 501, 100)\n",
    "}\n",
    "\n",
    "# получается достаточно много комбинаций гиперпараметров при переборе\n",
    "# будем использовать RandomizedSearchCV, он работает на много быстрее\n",
    "gs_lr = RandomizedSearchCV(\n",
    "    pipline_lr, \n",
    "    param_distributions=param_grid_lr, \n",
    "    scoring='f1', \n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "gs_lr.fit(X_train, y_train)\n",
    "\n",
    "gs_lr_best_score = gs_lr.best_score_\n",
    "gs_lr_best_params = gs_lr.best_params_\n",
    "print(f'best_F1: {gs_lr_best_score}')\n",
    "print(f'best_params: {gs_lr_best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель показала значение метрики f1, соответствующей условиям задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_F1: 0.19441543738671724\n",
      "best_params: {'randomforestclassifier__n_estimators': 10, 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__max_features': 18, 'randomforestclassifier__max_depth': 5}\n",
      "CPU times: total: 7.33 s\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer =  TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# model_rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "model_rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "pipline_rf = make_pipeline(vectorizer, model_rf)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'randomforestclassifier__n_estimators': range(10, 20, 2),\n",
    "    'randomforestclassifier__max_depth': range(3, 11, 2),\n",
    "    'randomforestclassifier__max_features': range(8,19,2),\n",
    "    'randomforestclassifier__min_samples_split': (2, 3),\n",
    "    'randomforestclassifier__min_samples_leaf': (1, 2, 3)\n",
    "}\n",
    "\n",
    "\n",
    "# получается достаточно много комбинаций гиперпараметров при переборе\n",
    "# будем использовать RandomizedSearchCV, он работает на много быстрее\n",
    "gs_rf = RandomizedSearchCV(\n",
    "    pipline_rf, \n",
    "    param_distributions=param_grid_rf, \n",
    "    scoring='f1', \n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "gs_rf_best_score = gs_rf.best_score_\n",
    "gs_rf_best_params = gs_rf.best_params_\n",
    "print(f'best_F1: {gs_rf_best_score}')\n",
    "print(f'best_params: {gs_rf_best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель показала низкое значение метрики f1 и не подходит по условиям задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_F1: nan\n",
      "best_params: {'catboostclassifier__verbose': False, 'catboostclassifier__learning_rate': 0.1, 'catboostclassifier__iterations': 223}\n",
      "CPU times: total: 10min 14s\n",
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer =  TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "model_cb = CatBoostClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "pipline_cb = make_pipeline(vectorizer, model_cb)\n",
    "\n",
    "param_grid_cb = {'catboostclassifier__learning_rate':[x/10 for x in range(1, 3)], \n",
    "                 'catboostclassifier__iterations':[223], \n",
    "                 'catboostclassifier__verbose':[False]\n",
    "                }\n",
    "\n",
    "# получается достаточно много комбинаций гиперпараметров при переборе\n",
    "# будем использовать RandomizedSearchCV, он работает на много быстрее\n",
    "gs_cb = RandomizedSearchCV(\n",
    "    pipline_cb,\n",
    "    param_distributions=param_grid_cb,\n",
    "    scoring='f1', \n",
    "    n_jobs=-1,\n",
    "    cv = 3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "gs_cb.fit(X_train, y_train)\n",
    "# , eval_metric='rmse', categorical_feature=cat_features_ohe+cat_features_oe\n",
    "\n",
    "gs_cb_best_score = gs_cb.best_score_\n",
    "gs_cb_best_params = gs_cb.best_params_\n",
    "print(f'best_F1: {gs_cb_best_score}')\n",
    "print(f'best_params: {gs_cb_best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель показала значение метрики f1, соответствующее условиям задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшие показатели метрики F1 показала модель LogisticRegression, проверим её на тестовой выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_final shape: (119469, 134807)\n",
      "X_test_final shape: (39823, 134807)\n",
      "F1 на тестовой выборке: 0.7713103448275863\n"
     ]
    }
   ],
   "source": [
    "vectorizer =  TfidfVectorizer(stop_words=stop_words)\n",
    "X_train_final = vectorizer.fit_transform(X_train)\n",
    "X_test_final = vectorizer.transform(X_test)\n",
    "print(f'X_train_final shape: {X_train_final.shape}')\n",
    "print(f'X_test_final shape: {X_test_final.shape}')\n",
    "\n",
    "model = LogisticRegression(max_iter=200, C=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "predicted = model.predict(X_test_final)\n",
    "print(f'F1 на тестовой выборке: {f1_score(y_test, predicted)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель подтвердила высокое качество на тестовой выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы обработали данные, очистили текст комментария от лишних символов.\n",
    "\n",
    "Затем разбили текст на слова и привели каждое слово к его начальной форме.\n",
    "\n",
    "После этого выполнили векторизацию текстов и на основе полученных векторов провели обучение моделей.\n",
    "\n",
    "Наилучшей моделью оказалась **LogisticRegression**"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 112,
    "start_time": "2023-04-12T15:58:39.422Z"
   },
   {
    "duration": 3612,
    "start_time": "2023-04-12T16:11:13.313Z"
   },
   {
    "duration": 2286,
    "start_time": "2023-04-12T16:11:20.870Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-12T16:11:31.767Z"
   },
   {
    "duration": 35,
    "start_time": "2023-04-12T16:11:35.350Z"
   },
   {
    "duration": 223,
    "start_time": "2023-04-12T16:11:36.248Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-12T16:11:37.741Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-12T16:11:39.905Z"
   },
   {
    "duration": 30,
    "start_time": "2023-04-12T16:11:40.181Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-12T16:11:43.420Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-12T16:11:44.436Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-12T16:11:49.148Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-12T16:11:52.211Z"
   },
   {
    "duration": 246,
    "start_time": "2023-04-12T16:11:54.158Z"
   },
   {
    "duration": 1124691,
    "start_time": "2023-04-12T16:12:21.109Z"
   },
   {
    "duration": 1990,
    "start_time": "2023-04-12T16:31:20.659Z"
   },
   {
    "duration": 43,
    "start_time": "2023-04-12T16:31:48.365Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-12T16:31:59.065Z"
   },
   {
    "duration": 2674,
    "start_time": "2023-04-12T16:32:27.769Z"
   },
   {
    "duration": 2645,
    "start_time": "2023-04-12T16:32:30.444Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-12T16:32:33.090Z"
   },
   {
    "duration": 50,
    "start_time": "2023-04-12T16:32:33.106Z"
   },
   {
    "duration": 246,
    "start_time": "2023-04-12T16:32:33.157Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-12T16:32:33.404Z"
   },
   {
    "duration": 24,
    "start_time": "2023-04-12T16:32:33.415Z"
   },
   {
    "duration": 43,
    "start_time": "2023-04-12T16:32:33.440Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-12T16:32:33.484Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-12T16:32:33.492Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-12T16:32:33.504Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-12T16:32:33.511Z"
   },
   {
    "duration": 1130197,
    "start_time": "2023-04-12T16:32:33.522Z"
   },
   {
    "duration": 2027,
    "start_time": "2023-04-12T16:51:23.720Z"
   },
   {
    "duration": 2532,
    "start_time": "2023-04-12T21:17:59.278Z"
   },
   {
    "duration": 2099,
    "start_time": "2023-04-12T21:18:01.812Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-12T21:18:03.913Z"
   },
   {
    "duration": 25,
    "start_time": "2023-04-12T21:18:03.929Z"
   },
   {
    "duration": 265,
    "start_time": "2023-04-12T21:18:03.956Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-12T21:18:04.223Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-12T21:18:04.232Z"
   },
   {
    "duration": 38,
    "start_time": "2023-04-12T21:18:04.242Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-12T21:18:04.282Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-12T21:18:04.290Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-12T21:18:04.298Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-12T21:18:04.307Z"
   },
   {
    "duration": 1127028,
    "start_time": "2023-04-12T21:18:04.317Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-12T21:36:51.347Z"
   },
   {
    "duration": 68,
    "start_time": "2023-04-12T21:36:51.351Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-12T21:36:51.421Z"
   },
   {
    "duration": 4861949,
    "start_time": "2023-04-12T21:36:51.425Z"
   },
   {
    "duration": 257379,
    "start_time": "2023-04-12T22:57:53.376Z"
   },
   {
    "duration": 3190258,
    "start_time": "2023-04-12T23:02:10.756Z"
   },
   {
    "duration": 21,
    "start_time": "2023-04-12T23:55:21.015Z"
   },
   {
    "duration": 93535,
    "start_time": "2023-04-12T23:55:21.038Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
